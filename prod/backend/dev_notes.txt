dev notes for the express backend that is responsible to communicate with mongodb

the solution should be fully Dockerized eventually
but for now we want only following functionality:

provide JSON file with words         words_0001.json
provide JSON file with sentences     sentences_0001.json

there will be script that will seed those word and sentence data to mongo database

once the db is fully seeded, there will be some logic 
that will create appropriate word-sentence relationships

at this stage, the actual express backend should be able to call db and provide word-sentence payloads


but we still do not have audio
    {
        "vocabulary_japanese": "入場券",
        "vocabulary_simplified": "にゅうじょうけん",
        "vocabulary_english": "entrance ticket",
        "word_type": "Verb",
        "vocabulary_audio": "/audio/vocab/v_入場券.mp3",
        "p_tag": "JLPT_N3",
        "s_tag": "100"
      },
the path to audio is already defined in db, so that is not an issue
question is where will we put the audio files themselves, 
we could just rsync them to some dir on our prod server, so frontend can read them directly
no need for now to store audio in container and serve audio via backend or whatever
simple ansible playbook/rsync should suffice

so audio/pictures are not an issue
actually, the audio can be baked in the frontend component and nobody cares 
thats amazing solution, image is already like hundreds of megabytes, whats 200 more megabytes

AUDIO CREATION:
we take the same json files and send them to python scripts to create the actual audio files
and save them to specific dir with specific name, no issue here

-----------------------------------------------------------------------------------


DB SEEDING:
-----------

sequence of commands for seeding the database:

# creates database, seeds database with ALL words and ALL sentences, creates word-sentence relationships
node word_relationships_GPT.js      


# run backend
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/my_mongo_prototypes$ node my_server.js 
Example app listening on port 8000
Mongo Connection Open.


curl -X GET http://localhost:8000/api/v1/words
curl -X GET 'http://localhost:8000/api/v1/words?p_tag=JLPT_N3&s_tag=100'     //must use quotes
maybe sometimes port 7000 when in container, via nginx



---------------------------------------------------------------------------------------------------------------------
------------------------------------- grammar section ---------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

first we need to generate grammar points via GPT 4 API 
list of grammar points is in a file 
 


then script called --- calls API and saves output to a file called grammar_0001.json


we have grammarpoint generation scripts on Desktop in openai_API directory, we should copy scripts to backend, so they are together
make sure to add config.py to gitignore, so we do not commit API key



test grammar json file is in json_data/test_grammar_0001.json

now i want to put the test grammar data to mongodb

below script will seed all data in json_data directory that is in files called grammar_*.json
seed all grammar to database like:
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db$ node seed_grammar_to_db.js 
Mongo Connection Open.
finally block - closing db conn
finally block - executed

once grammar data provisioned in the database 
start local express server that can retrieve grammar data from the DB 
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db$ node my_server.js 

and retrieve collection as
curl -X GET 'http://localhost:8000/api/v1/grammars?p_tag=JLPT_N3&s_tag=10'

that is all pretty good, we just need to get nice naming of the 
audio files and then of course to generate them

audio paths:
so first we need to take json file with all the grammar points (list of dictionaries)
and we need to add key 'grammar_audio' to each jp sentence
this is done by script called 
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/bulk_vocab_sentences$ python3 add_correct_audio_path_for_grammar_payload.py <input_file> <output_file>

expected file names are 
input_file = "grammars.json"
output_file = "grammars_updated.json"

"grammars_updated.json"   - this is our final file, with this one we will also seed database
can be used for audio generation via TTS engine

so how do we create the audio files now?
we need a script for that 

python3 app_grammar.py grammars_updated.json

python3 app_grammar.py grammar_N3_full_alphabetical_0001.json

that is easy 


so what languages we have 
Japanese - grammar generated
Vietnamese - grammar generated
Mandarin - HSK grammar generated
Korean - TOPIK
Thai - CU-TFL raw file


lang_paths = {
    'jp': '/audio/japanese/grammar/',
    'th': '/audio/thai/grammar/',
    'kr': '/audio/korean/grammar/',
    'vn': '/audio/vietnamese/grammar/',
    'cn': '/audio/mandarin/grammar/'
}


ok, so now we have many files for different languages with

Vietnamese example - in coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/openai_API$
this file is corrected raw data in format produced by GPT
production-all_vietnamese_grammar_corrected.txt

our sed pipe will clean it up to a json file and then we can just make final touches manually (including trailing comma fix)
cat production-all_vietnamese_grammar_corrected.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > production-all_vietnamese_grammar_corrected.json
also sometimes last en key has trailing comma, remove it so json is valid
sed -i '/"en":/s/,[[:space:]]*$//' production-all_vietnamese_grammar_corrected.json

now we have valid json
lets add tags and audio path
python3 add_tags_to_payload.py VIET production-all_vietnamese_grammar_corrected.json production-all_vietnamese_grammar_corrected_w_tags.json
python3 add_correct_audio_path_for_grammar_payload.py production-all_vietnamese_grammar_corrected_w_tags.json production-all_vietnamese_grammar_corrected_w_tags_w_audio.json 'vn'

this generates basically our json file that we can seed to db and use to generate audio
so we can rename it to something like
grammar_vn_full_0001.json

in our same dir we have now mp3 gtts generator
the generated audio can be then copied directly to frontend public dir
python3 app_grammar_generate_mp3_files.py grammar_vn_full_0001.json 'vn'

----------------------------------------------------------------------------------------

it is very easy, now we can make the same for other languages

--- MANDARIN ---



GPT4_output_generated_HSK_1_grammar_list.txt
GPT4_output_generated_HSK_2_grammar_list.txt
GPT4_output_generated_HSK_3_grammar_list.txt
GPT4_output_generated_HSK_4_grammar_list.txt
GPT4_output_generated_HSK_5_grammar_list.txt
GPT4_output_generated_HSK_6_grammar_list.txt

cat GPT4_output_generated_HSK_1_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_generated_HSK_1.json       x
cat GPT4_output_generated_HSK_2_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_generated_HSK_2.json       x
cat GPT4_output_generated_HSK_3_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_generated_HSK_3.json       x
cat GPT4_output_generated_HSK_4_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_generated_HSK_4.json       x
cat GPT4_output_generated_HSK_5_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_generated_HSK_5.json       x
cat GPT4_output_generated_HSK_6_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_generated_HSK_6.json       x


python3 add_tags_to_payload.py HSK_1 GPT4_output_generated_HSK_1.json GPT4_output_generated_HSK_1_w_tags.json      x
python3 add_tags_to_payload.py HSK_2 GPT4_output_generated_HSK_2.json GPT4_output_generated_HSK_2_w_tags.json      x
python3 add_tags_to_payload.py HSK_3 GPT4_output_generated_HSK_3.json GPT4_output_generated_HSK_3_w_tags.json      x
python3 add_tags_to_payload.py HSK_4 GPT4_output_generated_HSK_4.json GPT4_output_generated_HSK_4_w_tags.json      x
python3 add_tags_to_payload.py HSK_5 GPT4_output_generated_HSK_5.json GPT4_output_generated_HSK_5_w_tags.json      x
python3 add_tags_to_payload.py HSK_6 GPT4_output_generated_HSK_6.json GPT4_output_generated_HSK_6_w_tags.json      x


python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_generated_HSK_1_w_tags.json grammar_cn_HSK_1_0001.json 'cn'    x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_generated_HSK_2_w_tags.json grammar_cn_HSK_2_0001.json 'cn'    x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_generated_HSK_3_w_tags.json grammar_cn_HSK_3_0001.json 'cn'    x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_generated_HSK_4_w_tags.json grammar_cn_HSK_4_0001.json 'cn'    x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_generated_HSK_5_w_tags.json grammar_cn_HSK_5_0001.json 'cn'    x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_generated_HSK_6_w_tags.json grammar_cn_HSK_6_0001.json 'cn'    x

final audio generation:

python3 app_grammar_generate_mp3_files.py grammar_cn_HSK_1_0001.json 'cn'      # DONE
python3 app_grammar_generate_mp3_files.py grammar_cn_HSK_2_0001.json 'cn'      # DONE
python3 app_grammar_generate_mp3_files.py grammar_cn_HSK_3_0001.json 'cn'      # DONE
python3 app_grammar_generate_mp3_files.py grammar_cn_HSK_4_0001.json 'cn'      # DONE
python3 app_grammar_generate_mp3_files.py grammar_cn_HSK_5_0001.json 'cn'      # DONE
python3 app_grammar_generate_mp3_files.py grammar_cn_HSK_6_0001.json 'cn'      # DONE




--- KOREAN ---

GPT4_output_TOPIK_1_grammar_list.txt
GPT4_output_TOPIK_2_grammar_list.txt
GPT4_output_TOPIK_3_grammar_list.txt
GPT4_output_TOPIK_4_grammar_list.txt
GPT4_output_TOPIK_5_grammar_list.txt
GPT4_output_TOPIK_6_grammar_list.txt


cat GPT4_output_TOPIK_1_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_TOPIK_1.json       x
cat GPT4_output_TOPIK_2_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_TOPIK_2.json       x
cat GPT4_output_TOPIK_3_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_TOPIK_3.json       x
cat GPT4_output_TOPIK_4_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_TOPIK_4.json       x
cat GPT4_output_TOPIK_5_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_TOPIK_5.json       x
cat GPT4_output_TOPIK_6_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_TOPIK_6.json       x




python3 add_tags_to_payload.py TOPIK_1 GPT4_output_TOPIK_1.json GPT4_output_TOPIK_1_w_tags.json     x
python3 add_tags_to_payload.py TOPIK_2 GPT4_output_TOPIK_2.json GPT4_output_TOPIK_2_w_tags.json     x
python3 add_tags_to_payload.py TOPIK_3 GPT4_output_TOPIK_3.json GPT4_output_TOPIK_3_w_tags.json     x
python3 add_tags_to_payload.py TOPIK_4 GPT4_output_TOPIK_4.json GPT4_output_TOPIK_4_w_tags.json     x
python3 add_tags_to_payload.py TOPIK_5 GPT4_output_TOPIK_5.json GPT4_output_TOPIK_5_w_tags.json     x
python3 add_tags_to_payload.py TOPIK_6 GPT4_output_TOPIK_6.json GPT4_output_TOPIK_6_w_tags.json     x


python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_TOPIK_1_w_tags.json grammar_kr_TOPIK_1_0001.json 'kr'      x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_TOPIK_2_w_tags.json grammar_kr_TOPIK_2_0001.json 'kr'      x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_TOPIK_3_w_tags.json grammar_kr_TOPIK_3_0001.json 'kr'      x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_TOPIK_4_w_tags.json grammar_kr_TOPIK_4_0001.json 'kr'      x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_TOPIK_5_w_tags.json grammar_kr_TOPIK_5_0001.json 'kr'      x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_TOPIK_6_w_tags.json grammar_kr_TOPIK_6_0001.json 'kr'      x

final audio generation:
python3 app_grammar_generate_mp3_files.py grammar_kr_TOPIK_1_0001.json 'kr'      # DONE
python3 app_grammar_generate_mp3_files.py grammar_kr_TOPIK_2_0001.json 'kr'      # DONE
python3 app_grammar_generate_mp3_files.py grammar_kr_TOPIK_3_0001.json 'kr'      # DONE
python3 app_grammar_generate_mp3_files.py grammar_kr_TOPIK_4_0001.json 'kr'      # DONE
python3 app_grammar_generate_mp3_files.py grammar_kr_TOPIK_5_0001.json 'kr'      # DONE
python3 app_grammar_generate_mp3_files.py grammar_kr_TOPIK_6_0001.json 'kr'      # DONE




--- THAI ---

GPT4_output_CU-TFL_1_grammar_list.txt
GPT4_output_CU-TFL_2_grammar_list.txt
GPT4_output_CU-TFL_3_grammar_list.txt
GPT4_output_CU-TFL_4_grammar_list.txt
GPT4_output_CU-TFL_5_grammar_list.txt


cat GPT4_output_CU-TFL_1_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_CU-TFL_1.json       x
cat GPT4_output_CU-TFL_2_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_CU-TFL_2.json       x
cat GPT4_output_CU-TFL_3_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_CU-TFL_3.json       x
cat GPT4_output_CU-TFL_4_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_CU-TFL_4.json       x
cat GPT4_output_CU-TFL_5_grammar_list.txt | sed 's/-----------------------------/,/g' | sed -e '/^keyword:/d; /^PROMPT:/d; s/RESPONSE://g' | sed '/"en":/s/,[[:space:]]*$//' > GPT4_output_CU-TFL_5.json       x









python3 add_tags_to_payload.py CU-TFL_1 GPT4_output_CU-TFL_1.json GPT4_output_CU-TFL_1_w_tags.json     x
python3 add_tags_to_payload.py CU-TFL_2 GPT4_output_CU-TFL_2.json GPT4_output_CU-TFL_2_w_tags.json     x
python3 add_tags_to_payload.py CU-TFL_3 GPT4_output_CU-TFL_3.json GPT4_output_CU-TFL_3_w_tags.json     x
python3 add_tags_to_payload.py CU-TFL_4 GPT4_output_CU-TFL_4.json GPT4_output_CU-TFL_4_w_tags.json     x
python3 add_tags_to_payload.py CU-TFL_5 GPT4_output_CU-TFL_5.json GPT4_output_CU-TFL_5_w_tags.json     x



python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_CU-TFL_1_w_tags.json grammar_th_CU-TFL_1_0001.json 'th'     x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_CU-TFL_2_w_tags.json grammar_th_CU-TFL_2_0001.json 'th'     x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_CU-TFL_3_w_tags.json grammar_th_CU-TFL_3_0001.json 'th'     x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_CU-TFL_4_w_tags.json grammar_th_CU-TFL_4_0001.json 'th'     x
python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_CU-TFL_5_w_tags.json grammar_th_CU-TFL_5_0001.json 'th'     x



final audio generation:

python3 app_grammar_generate_mp3_files.py grammar_th_CU-TFL_1_0001.json 'th'     #  d
python3 app_grammar_generate_mp3_files.py grammar_th_CU-TFL_2_0001.json 'th'     #  d
python3 app_grammar_generate_mp3_files.py grammar_th_CU-TFL_3_0001.json 'th'     #  d
python3 app_grammar_generate_mp3_files.py grammar_th_CU-TFL_4_0001.json 'th'     #  d
python3 app_grammar_generate_mp3_files.py grammar_th_CU-TFL_5_0001.json 'th'     #  d





TODO:
WE HAVE BUG IN AUDIO PATH CREATION SCRIPT, it preseves slashes, so the audio files have slashes in names, which is wrong 
        
        "th": "คุณ รู้จัก เขา ไหม ค่ะ ?",
        "romaji": "Khun roojak khao mai kha?",
        "en": "Do you know him, sir/madam?",
        "grammar_audio": "/audio/thai/grammar/s_ครับ/ค่ะ_khrap/kha_-_Polite_ending_particle_for_male/female_speakers_20231230_คุณ_รู้จัก_เขา_ไหม_ค่ะ_.mp3"

we need to redo all audio naming and audio creation stages, but later, now we do not have time for this
wooow, we missed this, that sucks

also getting these file too long errors:
Converted ฉัน ชอบ การ เรียน ภาษา อังกฤษ โดยเฉพาะ การ เขียน วรรณกรรม to mp3 and saved to grammar_2023-12-31-12-17-38/s_โดยเฉพาะ_dooy_chêephaaw_-_Especially_particularly_20231230_ฉัน_ชอบ_การ_เรียน_ภาษา_อังกฤษ_โดยเฉพาะ_การ_เขียน_วรรณกรรม.mp3
Error: Could not open file 'grammar_2023-12-31-12-17-38/s_โดยเฉพาะ_dooy_chêephaaw_-_Especially_particularly_20231230_เขา_ชื่นชอบ_เที่ยว_เมือง_ต่าง_ๆ_โดยเฉพาะ_ที่_เป็น_อนุสาวรีย์_ประวัติศาสตร์.mp3': File name too long
Converted เขา ชื่นชอบ เที่ยว เมือง ต่าง ๆ โดยเฉพาะ ที่ เป็น อนุสาวรีย์ ประวัติศาสตร์ to mp3 and saved to grammar_2023-12-31-12-17-38/s_โดยเฉพาะ_dooy_chêephaaw_-_Especially_particularly_20231230_เขา_ชื่นชอบ_เที่ยว_เมือง_ต่าง_ๆ_โดยเฉพาะ_ที่_เป็น_อนุสาวรีย์_ประวัติศาสตร์.mp3

I do not see it saved as mp3, so it failed
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/openai_API/grammar_2023-12-31-12-17-38$ ls | grep s_โดยเฉพาะ_dooy_chêephaaw_-
s_โดยเฉพาะ_dooy_chêephaaw_-_Especially_particularly_20231230_ฉัน_ชอบ_การ_เรียน_ภาษา_อังกฤษ_โดยเฉพาะ_การ_เขียน_วรรณกรรม.mp3
s_โดยเฉพาะ_dooy_chêephaaw_-_Especially_particularly_20231230_ฉัน_รัก_ผัก_ใน_ทุก_รูป_แบบ_โดยเฉพาะ_ที่_เป็น_หน่อไม้_ผัด_พริก.mp3
s_โดยเฉพาะ_dooy_chêephaaw_-_Especially_particularly_20231230_วันนี้_มี_อากาศ_ที่_ดี_โดยเฉพาะ_การ_มี_แดด_อย่าง_ไม่_เกินไป.mp3
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/openai_API/grammar_2023-12-31-12-17-38$ 

TODO, we need to have some kind of name length limiter in naming script

if we limit it to take just strings before the bracket and not using data, we should be fine 
"title": "โดยเฉพาะ (dooi chêphâw) - Especially, particularly",
maybe we can even keep the date

just needs minor improvements




--------------------------------------------
how do we generate all the grammar points with gpt in the first place?

(.env) coil@coil-VM:~/Desktop/openai_API$ ./grammar_wrapper.sh N3_grammar_week_1-6_alphabetical.txt GPT4_output_N3_grammar_week_1-6_alphabetical.txt
we have moved and streamlined the code to main repo 

modify the app_grammar_jap_kword.py a bit: change JLPT Nx level, so GPT understands complexity of explanations

(.env) coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/openai_API$ 
./grammar_wrapper.sh N3_grammar_week_1-6_alphabetical.txt GPT4_output_N3_grammar_week_1-6_alphabetical.txt   #DONE
./grammar_wrapper.sh N4_grammar_week_1-4_alphabetical.txt GPT4_output_N4_grammar_week_1-4_alphabetical.txt   #DONE
./grammar_wrapper.sh N2_grammar_week_1-8_alphabetical.txt GPT4_output_N2_grammar_week_1-8_alphabetical.txt   #DONE
./grammar_wrapper.sh N5_grammar_week_3-5_alphabetical.txt GPT4_output_N5_grammar_week_3-5_alphabetical.txt   #DONE
./grammar_wrapper.sh N1_grammar_week_1-8_alphabetical.txt GPT4_output_N1_grammar_week_1-8_alphabetical.txt   #DONE



so now we have the text files with raw GPT output (.txt)
now manually clean it up to make it regular .json file 

now call (create) script that will add parent and child tags to json 
#python3 add_tags_to_payload.py JLPT_N3 missing_tags.json missing_tags_updated.json


this will add the p_tag and s_tag

#python3 add_tags_to_payload.py JLPT_N4 GPT4_output_N4_grammar_week_1-4_alphabetical.json GPT4_output_N4_grammar_week_1-4_alphabetical_w_tags.json    #DONE
#python3 add_tags_to_payload.py JLPT_N2 GPT4_output_N2_grammar_week_1-8_alphabetical.json GPT4_output_N2_grammar_week_1-8_alphabetical_w_tags.json    #DONE
#python3 add_tags_to_payload.py JLPT_N5 GPT4_output_N5_grammar_week_3-5_alphabetical.json GPT4_output_N5_grammar_week_3-5_alphabetical_w_tags.json    #DONE
#python3 add_tags_to_payload.py JLPT_N1 GPT4_output_N1_grammar_week_1-8_alphabetical.json GPT4_output_N1_grammar_week_1-8_alphabetical_w_tags.json    #DONE


now correct audio path 
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/bulk_vocab_sentences$ 
#python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_N4_grammar_week_1-4_alphabetical_w_tags.json GPT4_output_N4_grammar_week_1-4_alphabetical_w_tags_w_audio.json
#python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_N2_grammar_week_1-8_alphabetical_w_tags.json GPT4_output_N2_grammar_week_1-8_alphabetical_w_tags_w_audio.json
#python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_N5_grammar_week_3-5_alphabetical_w_tags.json GPT4_output_N5_grammar_week_3-5_alphabetical_w_tags_w_audio.json
#python3 add_correct_audio_path_for_grammar_payload.py GPT4_output_N1_grammar_week_1-8_alphabetical_w_tags.json GPT4_output_N1_grammar_week_1-8_alphabetical_w_tags_w_audio.json


so now we have gold JSON 
we can use it for generating the actual audio
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/python_audio_creation$ python3 app_grammar.py grammar_N1_full_alphabetical_0001.json 

rename our gold file to something more readable like and generate audio
#python3 app_grammar.py grammar_N4_full_alphabetical_0001.json
#python3 app_grammar.py grammar_N2_full_alphabetical_0001.json
#python3 app_grammar.py grammar_N5_full_alphabetical_0001.json
#python3 app_grammar.py grammar_N1_full_alphabetical_0001.json

easy




---------------------------------------------------------------------------------------------------------------------
------------------------------------- Vietnamese grammar section ----------------------------------------------------
---------------------------------------------------------------------------------------------------------------------

lets just generate audio files 

python3 app_grammar_generate_mp3_files.py grammar_vn_all_corrected_0001.json 'vn'     #  p












---------------------------------------------------------------------------------------------------------------------
---------------------------------- seed various lang grammar to db --------------------------------------------------
---------------------------------------------------------------------------------------------------------------------


so now we have the main json file generated
we have new grammar seeding script
seed_grammar_to_db_other_langs.js    this one will seed grammar for all langs besides jap


and it will take data from 
















---------------------------------------------------------------------------------------------------------------------
------------------------------------- Vocabulary section ------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------
pro tip (randomize line order of vocabulary):
shuf filename.txt > randomized_filename.txt

Tango
N3 i-adjectives    ~30
N3 na-adjectives   ~50
N3 verbs           ~250
so we need to join verbs file to one, randomize and then split into 5 files with similar amount of lines

cat openai_N3_tango_verbs_TOTAL.txt | shuf | split -l 51 - openai_N3_tango_verbs_                                            DONE

we start with manually written .txt file with vocabulary in 'openai_API' dir
cat openai_N3_tango_p210-213_i-adjectives_1.txt    | shuf > openai_N3_tango_p210-213_i-adjectives_1_shuffled.txt             DONE
cat openai_N3_tango_p214-223_na-adjectives_1.txt   | shuf > openai_N3_tango_p214-223_na-adjectives_1_shuffled.txt            DONE
cat openai_N3_tango_verbs_TOTAL.txt                | shuf > openai_N3_tango_verbs_TOTAL_shuffled.txt                         DONE



浅い| あさい | shallow (い adj.) |  |  |  |
偉い|えらい  | great, admirable (い adj.) |  |  |  |
おかしい|  | funny (い adj.) |  |  |  |

then we feed this file to GPT4



so we get gpt output .txt file called (with sentences, this does not need to be shuffled)
output_openai_N3_tango_p210-213_i-adjectives_1.txt

output_openai_N3_tango_p210-213_i-adjectives_1.txt
output_openai_N3_tango_p214-223_na-adjectives_1.txt
output_openai_N3_tango_verbs_TOTAL.txt

-----------------------------
CLI param:  きつい
--- Response text: ---
1. きつい | Kanji: 寒い | Romaji: Samui | Translation: It's cold | 
2. きつい | Kanji: 厳しい | Romaji: Kibishii | Translation: It's strict | 
3. きつい | Kanji: 激しい | Romaji: Hageshii | Translation: It's intense | 
4. きつい | Kanji: 緊張した | Romaji: Kinchou shita | Translation: I'm tense | 
5. きつい | Kanji: 強烈な | Romaji: Kyouryoku na| Translation: It's intense|
-----------------------------

CALL GPT4 as ('openai_API' dir, .env activated):
./wrapper.sh openai_N3_tango_p210-213_i-adjectives_1.txt output_openai_N3_tango_p210-213_i-adjectives_1.txt      # DONE
./wrapper.sh openai_N3_tango_p214-223_na-adjectives_1.txt output_openai_N3_tango_p214-223_na-adjectives_1.txt    # DONE
./wrapper.sh openai_N3_tango_verbs_TOTAL.txt output_openai_N3_tango_verbs_TOTAL.txt                              # DONE













once we have the output file, we need to get actual json file so we can feed it to database 

in 'bulk_vocab_sentences' dir
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/bulk_vocab_sentences$ 

convert vocab .txt file to json (this needs to be shuffled) (also adds audio path)
for database feeding (shuffled)
./convert_words_txt_to_json.sh openai_N3_tango_verbs_aa.txt openai_N3_tango_verbs_xaa.json "Verb" "JLPT_N3" "verbs-1"
./convert_words_txt_to_json.sh openai_N3_tango_verbs_ab.txt openai_N3_tango_verbs_xab.json "Verb" "JLPT_N3" "verbs-2"
./convert_words_txt_to_json.sh openai_N3_tango_verbs_ac.txt openai_N3_tango_verbs_xac.json "Verb" "JLPT_N3" "verbs-3"
./convert_words_txt_to_json.sh openai_N3_tango_verbs_ad.txt openai_N3_tango_verbs_xad.json "Verb" "JLPT_N3" "verbs-4"
./convert_words_txt_to_json.sh openai_N3_tango_verbs_ae.txt openai_N3_tango_verbs_xae.json "Verb" "JLPT_N3" "verbs-5"

./convert_words_txt_to_json.sh openai_N3_tango_verbs_TOTAL.txt openai_N3_tango_verbs_TOTAL.json "Verb" "JLPT_N3" "verb"
./convert_words_txt_to_json.sh openai_N3_tango_p210-213_i-adjectives_1_shuffled.txt openai_N3_tango_p210-213_i-adjectives_1_shuffled.json "Adjective" "JLPT_N3" "i-adjective"
./convert_words_txt_to_json.sh openai_N3_tango_p214-223_na-adjectives_1_shuffled.txt openai_N3_tango_p214-223_na-adjectives_1_shuffled.json "Adjective" "JLPT_N3" "na-adjective"


and convert sentence .txt file to json (doesnt have to be shuffled) (this also adds audio path)
./convert_sentences_txt_to_json.sh 'output_openai_N3_tango_verbs_TOTAL.txt' 'output_openai_N3_tango_verbs_TOTAL.json'
./convert_sentences_txt_to_json.sh 'output_openai_N3_tango_p210-213_i-adjectives_1.txt' 'output_openai_N3_tango_p210-213_i-adjectives_1.json'
./convert_sentences_txt_to_json.sh 'output_openai_N3_tango_p214-223_na-adjectives_1.txt' 'output_openai_N3_tango_p214-223_na-adjectives_1.json'


then rename files to something simple and copy these files to 'json_data' directory
these will now be json gold files, ready for audio generation 


how to generate audio ('python_audio_creation' dir)
python3 app_words.py openai_N3_tango_verbs_TOTAL.json                            # DONE
python3 app_words.py openai_N3_tango_p210-213_i-adjectives_1_shuffled.json       # DONE
python3 app_words.py openai_N3_tango_p214-223_na-adjectives_1_shuffled.json      # DONE


python3 app_sentences.py output_openai_N3_tango_verbs_TOTAL.json                 # DONE
python3 app_sentences.py output_openai_N3_tango_p210-213_i-adjectives_1.json     # DONE
python3 app_sentences.py output_openai_N3_tango_p214-223_na-adjectives_1.json    # DONE




--------------------------------------------------------------------------------------------------------------------------------------
------------------------------------- Essential 600 verbs book section ---------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------

some of the verbs will be the same as we already have elsewhere, 
so we will need to create separate document tables for the essential verbs 
in order to avoid duplicities
so separate table for words and separate table for sentences, so we can join easily
we should employ vocab key separation every time
sentences might be joined no problem i guess, we will see
but separate verbs for sure

--- 
so, first of all, we have manually written .txt file
we take this huge file and will generate sentences via GPT

I typically take the .txt file with vocabulary from relevant JLPT dir 
and then copy it to openai_API dir
and make final touches there

then it is ready for sentence generation via GPT









pro tip (randomize line order of vocabulary):
shuf filename.txt > randomized_filename.txt

Tango
N3 i-adjectives    ~30

so we need to join verbs file to one, randomize and then split into 5 files with similar amount of lines

cat openai_N3_tango_verbs_TOTAL.txt | shuf | split -l 51 - openai_N3_tango_verbs_                                            DONE
cat essential_600_verbs_book.txt | shuf | split -l 51 - essential_600_verbs_                                                 DONE                  407 verbs
cat suru_verbs_essential_600_verbs_book.txt | shuf | split -l 51 - suru_verbs_essential_600_verbs_                           DONE                  295 verbs
(add .txt suffix manually)

we start with manually written .txt file with vocabulary in 'openai_API' dir
cat openai_N3_tango_p210-213_i-adjectives_1.txt    | shuf > openai_N3_tango_p210-213_i-adjectives_1_shuffled.txt             DONE




浅い|あさい | shallow (い adj.) |  |  |  |
偉い|えらい | great, admirable (い adj.) |  |  |  |
おかしい|おかしい  | funny (い adj.) |  |  |  |

then we feed this file to GPT4



so we get gpt output .txt file called (with sentences, this does not need to be shuffled)
output_openai_N3_tango_p210-213_i-adjectives_1.txt


-----------------------------
CLI param:  きつい
--- Response text: ---
1. きつい | Kanji: 寒い | Romaji: Samui | Translation: It's cold | 
2. きつい | Kanji: 厳しい | Romaji: Kibishii | Translation: It's strict | 
3. きつい | Kanji: 激しい | Romaji: Hageshii | Translation: It's intense | 
4. きつい | Kanji: 緊張した | Romaji: Kinchou shita | Translation: I'm tense | 
5. きつい | Kanji: 強烈な | Romaji: Kyouryoku na| Translation: It's intense|
-----------------------------

CALL GPT4 as ('openai_API' dir, .env activated, can be done on unshuffled .txt vocabulary file, easy): 
./wrapper.sh openai_N3_tango_p210-213_i-adjectives_1.txt output_openai_N3_tango_p210-213_i-adjectives_1.txt      # DONE
./wrapper.sh suru_verbs_essential_600_verbs_book.txt output_suru_verbs_essential_600_verbs_book.txt              # DONE
./wrapper.sh essential_600_verbs_book.txt output_essential_600_verbs_book.txt                                    # DONE
also controlled the output files manually
rest of the steps is quite complex, so best to do it over the weekend



once we have the output file, we need to get actual json file so we can feed it to database 

in 'bulk_vocab_sentences' dir
-----------------------------
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/bulk_vocab_sentences$ 

WORDS:
------
convert vocab .txt file to json (this needs to be shuffled) (also adds audio path)
for database feeding (shuffled)
./convert_words_txt_to_json.sh openai_N3_tango_verbs_aa.txt openai_N3_tango_verbs_xaa.json "Verb" "JLPT_N3" "verbs-1"  DONE
./convert_words_txt_to_json.sh openai_N3_tango_verbs_ab.txt openai_N3_tango_verbs_xab.json "Verb" "JLPT_N3" "verbs-2"  DONE
./convert_words_txt_to_json.sh openai_N3_tango_verbs_ac.txt openai_N3_tango_verbs_xac.json "Verb" "JLPT_N3" "verbs-3"  DONE
./convert_words_txt_to_json.sh openai_N3_tango_verbs_ad.txt openai_N3_tango_verbs_xad.json "Verb" "JLPT_N3" "verbs-4"  DONE
./convert_words_txt_to_json.sh openai_N3_tango_verbs_ae.txt openai_N3_tango_verbs_xae.json "Verb" "JLPT_N3" "verbs-5"  DONE

./convert_words_txt_to_json.sh essential_600_verbs_book.txt essential_600_verbs_book.json "Verb" "essential_600_verbs" "verbs-0"                               DONE  use for TTS API
./convert_words_txt_to_json.sh suru_verbs_essential_600_verbs_book.txt suru_verbs_essential_600_verbs_book.json "Verb" "suru_essential_600_verbs" "verbs-0"    DONE  use for TTS API

./convert_words_txt_to_json.sh essential_600_verbs_aa.txt essential_600_verbs_xaa.json "Verb" "essential_600_verbs" "verbs-1"         DONE
./convert_words_txt_to_json.sh essential_600_verbs_ab.txt essential_600_verbs_xab.json "Verb" "essential_600_verbs" "verbs-2"         DONE
./convert_words_txt_to_json.sh essential_600_verbs_ac.txt essential_600_verbs_xac.json "Verb" "essential_600_verbs" "verbs-3"         DONE
./convert_words_txt_to_json.sh essential_600_verbs_ad.txt essential_600_verbs_xad.json "Verb" "essential_600_verbs" "verbs-4"         DONE
./convert_words_txt_to_json.sh essential_600_verbs_ae.txt essential_600_verbs_xae.json "Verb" "essential_600_verbs" "verbs-5"         DONE
./convert_words_txt_to_json.sh essential_600_verbs_af.txt essential_600_verbs_xaf.json "Verb" "essential_600_verbs" "verbs-6"         DONE
./convert_words_txt_to_json.sh essential_600_verbs_ag.txt essential_600_verbs_xag.json "Verb" "essential_600_verbs" "verbs-7"         DONE
./convert_words_txt_to_json.sh essential_600_verbs_ah.txt essential_600_verbs_xah.json "Verb" "essential_600_verbs" "verbs-8"         DONE

./convert_words_txt_to_json.sh suru_verbs_essential_600_verbs_aa.txt suru_verbs_essential_600_verbs_xaa.json "Verb" "suru_essential_600_verbs" "verbs-1"    DONE
./convert_words_txt_to_json.sh suru_verbs_essential_600_verbs_ab.txt suru_verbs_essential_600_verbs_xab.json "Verb" "suru_essential_600_verbs" "verbs-2"    DONE
./convert_words_txt_to_json.sh suru_verbs_essential_600_verbs_ac.txt suru_verbs_essential_600_verbs_xac.json "Verb" "suru_essential_600_verbs" "verbs-3"    DONE
./convert_words_txt_to_json.sh suru_verbs_essential_600_verbs_ad.txt suru_verbs_essential_600_verbs_xad.json "Verb" "suru_essential_600_verbs" "verbs-4"    DONE
./convert_words_txt_to_json.sh suru_verbs_essential_600_verbs_ae.txt suru_verbs_essential_600_verbs_xae.json "Verb" "suru_essential_600_verbs" "verbs-5"    DONE
./convert_words_txt_to_json.sh suru_verbs_essential_600_verbs_af.txt suru_verbs_essential_600_verbs_xaf.json "Verb" "suru_essential_600_verbs" "verbs-6"    DONE



./convert_words_txt_to_json.sh openai_N3_tango_p210-213_i-adjectives_1_shuffled.txt openai_N3_tango_p210-213_i-adjectives_1_shuffled.json "Adjective" "JLPT_N3" "i-adjective"   DONE


SENTENCES:
----------
and convert sentence .txt file to json (doesnt have to be shuffled) (this also adds audio path)
./convert_sentences_txt_to_json.sh 'output_openai_N3_tango_p210-213_i-adjectives_1.txt' 'output_openai_N3_tango_p210-213_i-adjectives_1.json'       DONE example

./convert_sentences_txt_to_json.sh 'output_essential_600_verbs_book.txt' 'output_essential_600_verbs_book.json'
./convert_sentences_txt_to_json.sh 'output_suru_verbs_essential_600_verbs_book.txt' 'output_suru_verbs_essential_600_verbs_book.json'







then rename files to something simple and copy these files to 'json_data' directory
these will now be json gold files, ready for audio generation 

sentences might need some cleanup of paths when vocab has brackets and commas (multiple vocab reading)
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/json_data$ sed -i 's/）/_/g' sentences_output_essential_600_verbs_book.json sentences_output_suru_verbs_essential_600_verbs_book.json 
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/json_data$ sed -i 's/（/_/g' sentences_output_essential_600_verbs_book.json sentences_output_suru_verbs_essential_600_verbs_book.json 
#######coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/json_data$ sed -i 's/、/_/g' sentences_output_essential_600_verbs_book.json sentences_output_suru_verbs_essential_600_verbs_book.json well the comma is causing lots of issues in audio generation

they might still not get paired well with vocab key, but we will leave this for later, it is not too many of them

AUDIO GENERATION:
-----------------
how to generate audio ('python_audio_creation' dir)
copy the gold json files to audio creation dir

WORDS
python3 app_words.py openai_N3_tango_p210-213_i-adjectives_1_shuffled.json            # DONE
python3 app_words.py words_essential_600_verbs_book_TOTAL.json                        # DONE
python3 app_words.py words_suru_verbs_essential_600_verbs_book_TOTAL.json             # DONE



SENTENCES
python3 app_sentences.py output_openai_N3_tango_p210-213_i-adjectives_1.json          # DONE
python3 app_sentences.py sentences_output_essential_600_verbs_book.json               # p
python3 app_sentences.py sentences_output_suru_verbs_essential_600_verbs_book.json    # p




DB SEEDING:
-----------
now we need to seed it into database somehow, at first, lets seed it to the same database, we just set vocab uniqueness to false

but we will also add _ underscore after new 600 book verbs so we can pair nicely with sentences for now, we can streamline later



sed -i 's/"vocabulary_japanese": "\(.*\)",$/"vocabulary_japanese": "\1_",/' words_essential_600_verbs_xa*
sed -i 's/"vocabulary_japanese": "\(.*\)",$/"vocabulary_japanese": "\1_",/' words_suru_verbs_essential_600_verbs_xa*

coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/json_data$ sed -i 's/"key": "\(.*\)"$/"key": "\1_"/' sentences_output_essential_600_verbs_book.json 
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/json_data$ sed -i 's/"key": "\(.*\)"$/"key": "\1_"/' sentences_output_suru_verbs_essential_600_verbs_book.json



------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------- Tanos JLPT N5-N1 Vocabulary --------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------

we still need to review the tanos data, sometimes I see there TODO markers and unfinished stuff

for now this is just vocabulary without sentences , but we can generate sentences later

so we have source data under JLPT/JLPT_tanos_data
it is in .csv files that look like
JLPT_N3_tanos_vocab_list.csv   and similar

印象|いんしょう|impression
引退|いんたい|retire

process files in bulk dir
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/bulk_vocab_sentences$

convert to json in following way 
(the s_tag is dynamic 100, 200, 300, ..., word type is left as nan)
./convert_words_txt_to_json_tanos.sh JLPT_N5_tanos_vocab_list.csv openai_JLPT_N5_tanos_vocab_list.json "nan" "JLPT_N5" "nan"
./convert_words_txt_to_json_tanos.sh JLPT_N4_tanos_vocab_list.csv openai_JLPT_N4_tanos_vocab_list.json "nan" "JLPT_N4" "nan"
./convert_words_txt_to_json_tanos.sh JLPT_N3_tanos_vocab_list.csv openai_JLPT_N3_tanos_vocab_list.json "nan" "JLPT_N3" "nan"
./convert_words_txt_to_json_tanos.sh JLPT_N2_tanos_vocab_list.csv openai_JLPT_N2_tanos_vocab_list.json "nan" "JLPT_N2" "nan"
./convert_words_txt_to_json_tanos.sh JLPT_N1_tanos_vocab_list.csv openai_JLPT_N1_tanos_vocab_list.json "nan" "JLPT_N1" "nan"

we also need to feed this into a db 
best way is to put is to separate collection, so we can query it independently
it is better this way, tanos data should ideally be separate
and later, once we have sentences, we can just make joins in that collection

example contents:
[
      {
        "vocabulary_japanese": "会う",
        "vocabulary_simplified": "あう",
        "vocabulary_english": "to meet",
        "word_type": "nan",
        "vocabulary_audio": "/audio/vocab/v_会う.mp3",
        "p_tag": "JLPT_N5",
        "s_tag": "100"
      },

How to seed DB:
---------------

node seed_tanos_vocab_to_db.js   # seeds full N5-N1 tanos vocab to separate db collection in our common db
it will look to .json files in json_data dir for files starting with 'wordsTanos_'
so it will be best to rename our json files straight away during creation, so we do not need to rename them




now we just need to generate audio
in 
coil@coil-VM:~/Desktop/zen-lingo-website/prod/backend/express/express-db/python_audio_creation$ 

python3 app_words.py openai_JLPT_N5_tanos_vocab_list.json  # done        
python3 app_words.py openai_JLPT_N4_tanos_vocab_list.json  # done 
python3 app_words.py openai_JLPT_N3_tanos_vocab_list.json  # done 
python3 app_words.py openai_JLPT_N2_tanos_vocab_list.json  # done 
python3 app_words.py openai_JLPT_N1_tanos_vocab_list.json  # done


generating audio is very easy 

we just then need to copy it to proper frontend dir /audio/vocab
best to do it manually in linux gui, vs code is too clunky for that

TODO
well, we could generate sentences as well for all the vocab straight away
since then DB work and API calls will be common, so I do not want to work on this twice
generate like just 2 sentences for each word, otherwise this would be super expensive to correct
and maybe just one sentence for N1, it would be too many sentences otherwise
I do not want to pay much


